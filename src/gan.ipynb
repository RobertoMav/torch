{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3332de89",
   "metadata": {},
   "source": [
    "# GENERATOR\n",
    "We need to compute:\n",
    "1. Sample a N(100) vector\n",
    "2. run forward pass\n",
    "3. calculate loss\n",
    "4. update weights\n",
    "\n",
    "# Discriminator\n",
    "1. Sample 16 MNIST data\n",
    "2. Sample 16 generator images\n",
    "3. Label and shuffle\n",
    "4. Forward\n",
    "5. Loss\n",
    "6. Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5b205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Generator, GeneratorMLP, Discriminator\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f174ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled z: torch.Size([16, 100])\n",
      "Generated images: torch.Size([16, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11f67d160>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "SAMPLE_Z_SIZE: int = 100\n",
    "BATCH_SIZE: int = 16\n",
    "sampled_z: torch.Tensor = torch.normal(mean=0, std=1, size=(BATCH_SIZE, SAMPLE_Z_SIZE))\n",
    "print(f\"Sampled z: {sampled_z.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "generator = GeneratorMLP()\n",
    "generated_images = generator(sampled_z)\n",
    "\n",
    "print(f\"Generated images: {generated_images.shape}\")\n",
    "plt.imshow(generated_images[0].detach().numpy().reshape(28, 28), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cb30f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return torch.mean(y_true) * torch.mean(y_pred)\n",
    "\n",
    "\n",
    "def generator_loss(y_pred):\n",
    "    return -torch.mean(y_pred)\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "generator = GeneratorMLP().to(device)\n",
    "\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb8557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(generator, discriminator, real_images, sampled_z, discriminator_optimizer, generator_optimizer, train_generator=False, device=device,):\n",
    "    discriminator_optimizer.zero_grad()\n",
    "    # 1. Move data to device\n",
    "    real_images = real_images.to(device) # [16, 1, 28, 28]\n",
    "\n",
    "    fake_images = generator(sampled_z) # [16, 1, 28, 28]\n",
    "\n",
    "    # 2. Forward pass\n",
    "    real_preds = discriminator(real_images) # [16, 1, 1, 1]\n",
    "    fake_preds = discriminator(fake_images) # [16, 1, 1, 1]\n",
    "\n",
    "    # 3. Calculate loss\n",
    "    real_loss = wasserstein_loss(real_preds, torch.ones_like(real_preds))\n",
    "    fake_loss = wasserstein_loss(fake_preds, -torch.ones_like(fake_preds))\n",
    "    total_loss = real_loss + fake_loss\n",
    "\n",
    "    # 4. Backward pass\n",
    "    total_loss.backward()\n",
    "    discriminator_optimizer.step()\n",
    "\n",
    "    if train_generator:\n",
    "        generator.zero_grad()\n",
    "        sampled_z = sampled_z.to(device) # [16, 100]\n",
    "        fake_images = generator(sampled_z) # [16, 1, 28, 28]\n",
    "        fake_preds = discriminator(fake_images) # [16, 1, 1, 1]\n",
    "        fake_loss = generator_loss(fake_preds)\n",
    "        fake_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "    return real_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d43b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmartin0/Developer/Study/DeepL/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from data_setup import create_dataloaders\n",
    "from utils import save_10_images\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_dataloader = create_dataloaders(data_dir=\"data\", batch_size=16, num_workers=1)\n",
    "\n",
    "for i, (real_images, _) in enumerate(train_dataloader):\n",
    "    print(real_images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83138fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sample_z_size = torch.normal(mean=0, std=1, size=(BATCH_SIZE, SAMPLE_Z_SIZE))\n",
    "\n",
    "def train(generator, discriminator, dataset, discriminator_optimizer, generator_optimizer, epochs=100, device=device,):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_images, _) in tqdm(enumerate(dataset)):\n",
    "            sampled_z = torch.normal(mean=0, std=1, size=(SAMPLE_Z_SIZE, SAMPLE_Z_SIZE)).to(device)\n",
    "            if i % 5 == 0:\n",
    "                train_generator = True\n",
    "            else:\n",
    "                train_generator = False\n",
    "            train_step(generator, discriminator, real_images, sampled_z, discriminator_optimizer, generator_optimizer, train_generator, device)\n",
    "            if i % 100 == 0:\n",
    "                save_10_images(generator(sampled_z), (epoch+1)*i)\n",
    "                # Print images\n",
    "                plt.imshow(real_images[0].squeeze(), cmap=\"gray\")\n",
    "                print(f\"Epoch {epoch} completed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00afc4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(generator, discriminator, dataset, discriminator_optimizer, generator_optimizer, epochs, device)\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      8\u001b[39m         train_generator = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     11\u001b[39m     save_10_images(generator(sampled_z), (epoch+\u001b[32m1\u001b[39m)*i)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(generator, discriminator, real_images, sampled_z, discriminator_optimizer, generator_optimizer, train_generator, device)\u001b[39m\n\u001b[32m     22\u001b[39m     fake_loss.backward()\n\u001b[32m     23\u001b[39m     generator_optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train(generator, discriminator, train_dataloader, discriminator_optimizer, generator_optimizer, epochs=100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072f5f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument input is on cpu but expected on mps",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m save_10_images(\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Study/DeepL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Study/DeepL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Study/DeepL/src/model.py:70\u001b[39m, in \u001b[36mGeneratorMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     69\u001b[39m     x = x.view(-\u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.fc2(x)\n\u001b[32m     72\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.fc3(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Study/DeepL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Study/DeepL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Study/DeepL/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Study/DeepL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Study/DeepL/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Study/DeepL/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Tensor for argument input is on cpu but expected on mps"
     ]
    }
   ],
   "source": [
    "\n",
    "save_10_images(generator(torch.randn(16, 100)), 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
